{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "# LangGraph pour l'architecture en étapes/noeuds\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# LangChain pour la génération de recommandations\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Récuépration des variables d'environnement (clé API)\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "## Création d'un client OpenAI pour la génération de recommandations\n",
    "client = OpenAI()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "## Document en entrée\n",
    "json_file = \"Data/rapport.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sortie pour les anomalies\n",
    "class Anomalies(BaseModel):\n",
    "    metric: str = Field(description=\"Métrique ou élément où il y a une anomalie\")\n",
    "    value: Any = Field(description=\"La valeur qui pose problème\")\n",
    "    issue: str = Field(description=\"Les conséquences ou explications de la valeur problématique\")\n",
    "    \n",
    "## Sortie pour les recommendations\n",
    "class Recommendations(BaseModel):\n",
    "    anomalie: str = Field(description=\"Description d'anomalie.s rencontrée.S\")\n",
    "    suggestion: str = Field(description=\"Action.s suggérée.s pour optimiser l'infrastructure\")\n",
    "\n",
    "class RecommendationList(BaseModel):\n",
    "    recommendations: List[Recommendations] = Field(description=\"Une liste de recommandations d'optimisation basées sur les anomalies détectées.\")\n",
    "\n",
    "    \n",
    "## Définition de l'état du graphe : InputState pour la gestion de l'entrée et OutputState pour la gestion de la sortie\n",
    "class State(TypedDict):\n",
    "    input_path: str                 \n",
    "    input_data: Optional[List[Dict[str, Any]]] \n",
    "    anomalies: List[Anomalies]     \n",
    "    recommendations: RecommendationList\n",
    "    error: Optional[str]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fonctions pour chaque étape de l'architecture\n",
    "\n",
    "def data_ingestion(state):\n",
    "    \"\"\" Noeud d'ingestion des données \"\"\"\n",
    "    print(\"### Noeud en cours : Ingestion des données ###\")\n",
    "    print(\"\\tSTATE : \", state)\n",
    "    try :\n",
    "        if \".json\" in state[\"input_data\"] : \n",
    "            with open(state[\"input_data\"], 'r') as file :\n",
    "                data = json.load(file)\n",
    "            print(f\"Data ingestion complétée pour le fichier : { state['input_data'] }\\n\")\n",
    "\n",
    "        else :\n",
    "            data = state[\"input_data\"]\n",
    "            print(f\"Data ingestion complétée\")\n",
    "        return {**state, \"input_data\": data, \"error\": None}\n",
    "        \n",
    "    except Exception as e :\n",
    "        print(f\"Erreur pendant l'ingestion du fichier : {e}\\n\")\n",
    "        return {**state, \"input_data\": {}, \"error\": f\"Ingestion failed: {str(e)}\"}\n",
    "    \n",
    "\n",
    "def anomalies_detection(state):\n",
    "    \"\"\" Noeud de détection d'anomalies dans les donées \"\"\"\n",
    "    print(\"\\n\\n### Noeud en cours : Détection des anomalies ###\")\n",
    "    print(\"\\tSTATE : \", state)\n",
    "\n",
    "    if state[\"error\"] != None:\n",
    "        print(\"Erreur détectée dans un noeud précédent. Arrêt de la génération de recommandations\\n\")\n",
    "        return state\n",
    "    \n",
    "    if type(state[\"input_data\"]) == dict : \n",
    "        state[\"input_data\"] = [state[\"input_data\"]]\n",
    "    data = state[\"input_data\"]\n",
    "    anomalies_detected: List[Anomalies] = []\n",
    "        \n",
    "    for d in data :\n",
    "        if d[\"cpu_usage\"] > 80 :\n",
    "            anomalies_detected.append({\"metric\": \"cpu_usage\",\"value\": d[\"cpu_usage\"],\"issue\": \"High CPU Usage (> 80%)\"})\n",
    "        if d[\"memory_usage\"] > 80 :\n",
    "            anomalies_detected.append({\"metric\": \"memory_usage\",\"value\": d[\"memory_usage\"],\"issue\": \"High Memory Usage (> 80%)\"})\n",
    "\n",
    "        service_status = d[\"service_status\"]\n",
    "        for service, status in service_status.items():\n",
    "            if status != \"online\":\n",
    "                anomalies_detected.append({\"metric\": f\"service_status : {service}\", \"value\": status, \"issue\": f\"Service {service} is {status}\"})\n",
    "        \n",
    "    print(f\"{len(anomalies_detected)} anomalie.s détectée.s\")\n",
    "    print(\"\\n\\tANOMALIES DETECTED \", anomalies_detected)\n",
    "    state[\"anomalies\"] = anomalies_detected\n",
    "    return state\n",
    "\n",
    "def recommandations_generation(state):\n",
    "    \"\"\" Noeud de génération de recommandations à partir des anomalies détectées \"\"\"\n",
    "    print(\"\\n\\n### Noeud en cours : Génération de recommandations ###\")\n",
    "    print(\"STATE : \", state)\n",
    "    \n",
    "    if state[\"error\"] != None:\n",
    "        print(\"Erreur détectée dans un noeud précédent. Arrêt de la génération de recommandations\")\n",
    "        return state\n",
    "    \n",
    "    anomalies = state[\"anomalies\"]    \n",
    "    recommendations: List[Recommendations] = [] \n",
    "    parser = PydanticOutputParser(pydantic_object=RecommendationList)\n",
    "    prompt_template = ChatPromptTemplate.from_messages([(\"system\",\n",
    "                                                         \"Tu es un ingénieur infrastructure expert analysant des anomalies de monitoring. \"\n",
    "                                                         \"Ta tâche est de fournir des recommandations d'optimisation concrètes et actionnables basées sur les anomalies détectées,. \"\n",
    "                                                         \"Structure ta réponse exactement selon le schéma JSON fourni.\"\n",
    "                                                         \"\\n{format_instructions}\"), \n",
    "                                                         (\"human\", \n",
    "                                                          \"Voici les anomalies détectées sur l'infrastructure :\\n\"\n",
    "                                                          \"{anomaly_list}\\n\\n\"\n",
    "                                                          \"Génère une liste de recommandations pour traiter ces problèmes, dans un seul texte.  Pour chaque anomalie, fournis un seul objet `Recommendations` \"\n",
    "                                                          \"où le champ 'suggestion' regroupe toutes les actions proposées pour cette anomalie, séparées par un saut de ligne et listées par des numéros.\")\n",
    "                                                        ])\n",
    "        \n",
    "    chain = prompt_template | llm | parser\n",
    "    \n",
    "\n",
    "    if anomalies == []:\n",
    "        print(\"Aucune anomalie trouvée dans l'état. Pas de recommandations à générer.\")\n",
    "        return {**state, \"recommendations\": []}\n",
    "    \n",
    "    for a in anomalies:\n",
    "        formatted_anomalies = f\"- Métrique: {a['metric']}, Valeur: {a['value']}, Problème: {a['issue']}\"\n",
    "        try:\n",
    "            print(\"Appel du LLM (gpt-4o-mini) pour générer les recommandations...\")\n",
    "            response = chain.invoke({\n",
    "                \"format_instructions\": parser.get_format_instructions(), \n",
    "                \"anomaly_list\": formatted_anomalies\n",
    "            })\n",
    "            \n",
    "            # recommendations.append(response.recommendations)\n",
    "            recommendations.append({\"anomalie\" : response.recommendations[0].anomalie, \"suggestion\": response.recommendations[0].suggestion})\n",
    "            print(f\"Génération réussie de {len(recommendations)} recommandations.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'appel LLM ou du parsing de la réponse : {e}\")\n",
    "            return {**state, \"recommendations\": [], \"error\": f\"Génération de recommandations a échoué: {str(e)}\"}\n",
    "\n",
    "    state[\"recommendations\"] = recommendations\n",
    "    print(\"FINAL STATE : \", state)\n",
    "    return state\n",
    "\n",
    "def file_creation(state):\n",
    "    print(\"\\n\\n### Noeud en cours : Création d'un fichier avec les anomalies et les recommandations (état final du graphe) ###\")\n",
    "    print(\"FINAL STATE : \", state)\n",
    "\n",
    "    with open(\"Recommendations/recommandations.json\", \"w\") as outfile:\n",
    "        json.dump(state, outfile, indent=4, sort_keys=False, ensure_ascii=False)#.encode('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Définition du graphe\n",
    "# Initialisation du graphe\n",
    "workflow = StateGraph(State) #, input=InputState, output=OutputState)\n",
    "\n",
    "# Définition des noeuds du graphe\n",
    "workflow.add_node(\"ingestion\", data_ingestion)\n",
    "workflow.add_node(\"analyze\", anomalies_detection)\n",
    "workflow.add_node(\"recommend\", recommandations_generation)\n",
    "workflow.add_node(\"file_creation\", file_creation)\n",
    "\n",
    "# Début du graphe\n",
    "workflow.set_entry_point(\"ingestion\")\n",
    "\n",
    "# Définition des arêtes du graphe\n",
    "workflow.add_edge(\"ingestion\", \"analyze\")\n",
    "workflow.add_edge(\"analyze\", \"recommend\")\n",
    "# workflow.add_edge(\"recommend\", END)\n",
    "workflow.add_edge(\"recommend\", \"file_creation\")\n",
    "workflow.add_edge(\"file_creation\", END)\n",
    "\n",
    "# Compilation du graphe\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Noeud en cours : Ingestion des données ###\n",
      "\tSTATE :  {'input_data': {'timestamp': '2023-10-01T15:00:00Z', 'cpu_usage': 73, 'memory_usage': 79, 'latency_ms': 213, 'disk_usage': 77, 'network_in_kbps': 1506, 'network_out_kbps': 1618, 'io_wait': 6, 'thread_count': 165, 'active_connections': 73, 'error_rate': 0.04, 'uptime_seconds': 370800, 'temperature_celsius': 67, 'power_consumption_watts': 290, 'service_status': {'database': 'online', 'api_gateway': 'degraded', 'cache': 'degraded'}}}\n",
      "Data ingestion complétée\n",
      "\n",
      "\n",
      "### Noeud en cours : Détection des anomalies ###\n",
      "\tSTATE :  {'input_data': {'timestamp': '2023-10-01T15:00:00Z', 'cpu_usage': 73, 'memory_usage': 79, 'latency_ms': 213, 'disk_usage': 77, 'network_in_kbps': 1506, 'network_out_kbps': 1618, 'io_wait': 6, 'thread_count': 165, 'active_connections': 73, 'error_rate': 0.04, 'uptime_seconds': 370800, 'temperature_celsius': 67, 'power_consumption_watts': 290, 'service_status': {'database': 'online', 'api_gateway': 'degraded', 'cache': 'degraded'}}, 'error': None}\n",
      "2 anomalie.s détectée.s\n",
      "\n",
      "\tANOMALIES DETECTED  [{'metric': 'service_status : api_gateway', 'value': 'degraded', 'issue': 'Service api_gateway is degraded'}, {'metric': 'service_status : cache', 'value': 'degraded', 'issue': 'Service cache is degraded'}]\n",
      "\n",
      "\n",
      "### Noeud en cours : Génération de recommandations ###\n",
      "STATE :  {'input_data': [{'timestamp': '2023-10-01T15:00:00Z', 'cpu_usage': 73, 'memory_usage': 79, 'latency_ms': 213, 'disk_usage': 77, 'network_in_kbps': 1506, 'network_out_kbps': 1618, 'io_wait': 6, 'thread_count': 165, 'active_connections': 73, 'error_rate': 0.04, 'uptime_seconds': 370800, 'temperature_celsius': 67, 'power_consumption_watts': 290, 'service_status': {'database': 'online', 'api_gateway': 'degraded', 'cache': 'degraded'}}], 'anomalies': [{'metric': 'service_status : api_gateway', 'value': 'degraded', 'issue': 'Service api_gateway is degraded'}, {'metric': 'service_status : cache', 'value': 'degraded', 'issue': 'Service cache is degraded'}], 'error': None}\n",
      "Appel du LLM (gpt-4o-mini) pour générer les recommandations...\n",
      "Génération réussie de 1 recommandations.\n",
      "Appel du LLM (gpt-4o-mini) pour générer les recommandations...\n",
      "Génération réussie de 2 recommandations.\n",
      "FINAL STATE :  {'input_data': [{'timestamp': '2023-10-01T15:00:00Z', 'cpu_usage': 73, 'memory_usage': 79, 'latency_ms': 213, 'disk_usage': 77, 'network_in_kbps': 1506, 'network_out_kbps': 1618, 'io_wait': 6, 'thread_count': 165, 'active_connections': 73, 'error_rate': 0.04, 'uptime_seconds': 370800, 'temperature_celsius': 67, 'power_consumption_watts': 290, 'service_status': {'database': 'online', 'api_gateway': 'degraded', 'cache': 'degraded'}}], 'anomalies': [{'metric': 'service_status : api_gateway', 'value': 'degraded', 'issue': 'Service api_gateway is degraded'}, {'metric': 'service_status : cache', 'value': 'degraded', 'issue': 'Service cache is degraded'}], 'error': None, 'recommendations': [{'anomalie': 'Service api_gateway is degraded', 'suggestion': \"1. Vérifiez l'état des instances du service api_gateway pour identifier toute instance défaillante.\\n2. Augmentez les ressources (CPU, RAM) allouées au service si elles sont insuffisantes.\\n3. Consultez les journaux d'erreur pour déterminer la cause sous-jacente de la dégradation.\\n4. Si le problème persiste, envisagez de redémarrer le service ou de le remplacer par une nouvelle instance.\\n5. Alertes : mettez en place des alertes pour surveiller l'état du service afin de réagir plus rapidement à de futures dégradations.\"}, {'anomalie': 'Service cache is degraded', 'suggestion': \"1. Vérifiez les logs du service cache pour identifier les erreurs spécifiques.\\n2. Évaluer la charge actuelle sur le service et optimiser les configurations de mise en cache.\\n3. Redémarrez le service cache pour tenter de restaurer son état normal.\\n4. Vérifiez les connexions à la base de données ou autres dépendances pouvant affecter le service cache.\\n5. Envisagez d'augmenter les ressources allouées au service cache (CPU, mémoire) si les problèmes persistent.\"}]}\n",
      "\n",
      "\n",
      "### Noeud en cours : Création d'un fichier avec les anomalies et les recommandations (état final du graphe) ###\n",
      "FINAL STATE :  {'input_data': [{'timestamp': '2023-10-01T15:00:00Z', 'cpu_usage': 73, 'memory_usage': 79, 'latency_ms': 213, 'disk_usage': 77, 'network_in_kbps': 1506, 'network_out_kbps': 1618, 'io_wait': 6, 'thread_count': 165, 'active_connections': 73, 'error_rate': 0.04, 'uptime_seconds': 370800, 'temperature_celsius': 67, 'power_consumption_watts': 290, 'service_status': {'database': 'online', 'api_gateway': 'degraded', 'cache': 'degraded'}}], 'anomalies': [{'metric': 'service_status : api_gateway', 'value': 'degraded', 'issue': 'Service api_gateway is degraded'}, {'metric': 'service_status : cache', 'value': 'degraded', 'issue': 'Service cache is degraded'}], 'recommendations': [{'anomalie': 'Service api_gateway is degraded', 'suggestion': \"1. Vérifiez l'état des instances du service api_gateway pour identifier toute instance défaillante.\\n2. Augmentez les ressources (CPU, RAM) allouées au service si elles sont insuffisantes.\\n3. Consultez les journaux d'erreur pour déterminer la cause sous-jacente de la dégradation.\\n4. Si le problème persiste, envisagez de redémarrer le service ou de le remplacer par une nouvelle instance.\\n5. Alertes : mettez en place des alertes pour surveiller l'état du service afin de réagir plus rapidement à de futures dégradations.\"}, {'anomalie': 'Service cache is degraded', 'suggestion': \"1. Vérifiez les logs du service cache pour identifier les erreurs spécifiques.\\n2. Évaluer la charge actuelle sur le service et optimiser les configurations de mise en cache.\\n3. Redémarrez le service cache pour tenter de restaurer son état normal.\\n4. Vérifiez les connexions à la base de données ou autres dépendances pouvant affecter le service cache.\\n5. Envisagez d'augmenter les ressources allouées au service cache (CPU, mémoire) si les problèmes persistent.\"}], 'error': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_data': [{'timestamp': '2023-10-01T15:00:00Z',\n",
       "   'cpu_usage': 73,\n",
       "   'memory_usage': 79,\n",
       "   'latency_ms': 213,\n",
       "   'disk_usage': 77,\n",
       "   'network_in_kbps': 1506,\n",
       "   'network_out_kbps': 1618,\n",
       "   'io_wait': 6,\n",
       "   'thread_count': 165,\n",
       "   'active_connections': 73,\n",
       "   'error_rate': 0.04,\n",
       "   'uptime_seconds': 370800,\n",
       "   'temperature_celsius': 67,\n",
       "   'power_consumption_watts': 290,\n",
       "   'service_status': {'database': 'online',\n",
       "    'api_gateway': 'degraded',\n",
       "    'cache': 'degraded'}}],\n",
       " 'anomalies': [{'metric': 'service_status : api_gateway',\n",
       "   'value': 'degraded',\n",
       "   'issue': 'Service api_gateway is degraded'},\n",
       "  {'metric': 'service_status : cache',\n",
       "   'value': 'degraded',\n",
       "   'issue': 'Service cache is degraded'}],\n",
       " 'recommendations': [{'anomalie': 'Service api_gateway is degraded',\n",
       "   'suggestion': \"1. Vérifiez l'état des instances du service api_gateway pour identifier toute instance défaillante.\\n2. Augmentez les ressources (CPU, RAM) allouées au service si elles sont insuffisantes.\\n3. Consultez les journaux d'erreur pour déterminer la cause sous-jacente de la dégradation.\\n4. Si le problème persiste, envisagez de redémarrer le service ou de le remplacer par une nouvelle instance.\\n5. Alertes : mettez en place des alertes pour surveiller l'état du service afin de réagir plus rapidement à de futures dégradations.\"},\n",
       "  {'anomalie': 'Service cache is degraded',\n",
       "   'suggestion': \"1. Vérifiez les logs du service cache pour identifier les erreurs spécifiques.\\n2. Évaluer la charge actuelle sur le service et optimiser les configurations de mise en cache.\\n3. Redémarrez le service cache pour tenter de restaurer son état normal.\\n4. Vérifiez les connexions à la base de données ou autres dépendances pouvant affecter le service cache.\\n5. Envisagez d'augmenter les ressources allouées au service cache (CPU, mémoire) si les problèmes persistent.\"}],\n",
       " 'error': None}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"input_data\" : {\n",
    "    \"timestamp\": \"2023-10-01T15:00:00Z\",\n",
    "    \"cpu_usage\": 73,\n",
    "    \"memory_usage\": 79,\n",
    "    \"latency_ms\": 213,\n",
    "    \"disk_usage\": 77,\n",
    "    \"network_in_kbps\": 1506,\n",
    "    \"network_out_kbps\": 1618,\n",
    "    \"io_wait\": 6,\n",
    "    \"thread_count\": 165,\n",
    "    \"active_connections\": 73,\n",
    "    \"error_rate\": 0.04,\n",
    "    \"uptime_seconds\": 370800,\n",
    "    \"temperature_celsius\": 67,\n",
    "    \"power_consumption_watts\": 290,\n",
    "    \"service_status\": {\n",
    "      \"database\": \"online\",\n",
    "      \"api_gateway\": \"degraded\",\n",
    "      \"cache\": \"degraded\"}\n",
    "    }\n",
    "  }    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"input_data\" : json_file}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "agent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
